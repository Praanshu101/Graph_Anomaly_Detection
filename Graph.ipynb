{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cadc247d",
   "metadata": {},
   "source": [
    "## Graph Construction from Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f05b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 33176\n",
      "Number of edges: 804322\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "# File path\n",
    "file_path = 'Data/02-20-2018.csv'\n",
    "\n",
    "# Chunk size (adjust based on available RAM)\n",
    "chunk_size = 1000000\n",
    "\n",
    "# Create an empty directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Stream CSV in chunks\n",
    "for chunk in pd.read_csv(file_path, usecols=[\"Src IP\", \"Dst IP\", \"TotLen Fwd Pkts\", \"TotLen Bwd Pkts\", \"Label\"], \n",
    "                         chunksize=chunk_size, low_memory=False):\n",
    "\n",
    "    # Drop rows with missing IPs\n",
    "    chunk.dropna(subset=[\"Src IP\", \"Dst IP\"], inplace=True)\n",
    "\n",
    "    # Iterate over rows\n",
    "    for _, row in chunk.iterrows():\n",
    "        try:\n",
    "            src = row[\"Src IP\"]\n",
    "            dst = row[\"Dst IP\"]\n",
    "            fwd_bytes = float(row[\"TotLen Fwd Pkts\"])\n",
    "            bwd_bytes = float(row[\"TotLen Bwd Pkts\"])\n",
    "            \n",
    "            # Forward direction: src -> dst\n",
    "            if G.has_edge(src, dst):\n",
    "                G[src][dst][\"weight\"] += fwd_bytes\n",
    "            else:\n",
    "                G.add_edge(src, dst, weight=fwd_bytes)\n",
    "            \n",
    "            # Backward direction: dst -> src\n",
    "            if G.has_edge(dst, src):\n",
    "                G[dst][src][\"weight\"] += bwd_bytes\n",
    "            else:\n",
    "                G.add_edge(dst, src, weight=bwd_bytes)\n",
    "\n",
    "        except (KeyError, ValueError):\n",
    "            continue  # Skip problematic rows\n",
    "\n",
    "# Graph stats\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Save the graph\n",
    "with open(\"network_graph.pkl\", \"wb\") as f:\n",
    "    pickle.dump(G, f)\n",
    "\n",
    "# Optional: Save a lightweight graph summary\n",
    "summary = {\n",
    "    \"nodes\": G.number_of_nodes(),\n",
    "    \"edges\": G.number_of_edges(),\n",
    "}\n",
    "with open(\"graph_summary.txt\", \"w\") as f:\n",
    "    f.write(str(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e760a",
   "metadata": {},
   "source": [
    "### Computing egonet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba09d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bde3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"network_graph.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b25f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "egonet_features = {}\n",
    "\n",
    "for node in G.nodes():\n",
    "    neighbors = list(G.successors(node)) + list(G.predecessors(node))\n",
    "    egonet_nodes = set(neighbors + [node])\n",
    "    subgraph = G.subgraph(egonet_nodes).copy()\n",
    "\n",
    "    N_i = len(subgraph.nodes)\n",
    "    E_i = len(subgraph.edges)\n",
    "    W_i = sum([d[\"weight\"] for u, v, d in subgraph.edges(data=True)])\n",
    "    \n",
    "    # Weighted adjacency matrix and its top eigenvalue\n",
    "    W_matrix = nx.to_numpy_array(subgraph, weight=\"weight\")\n",
    "    if W_matrix.shape[0] > 1:\n",
    "        lambda_w = max(np.linalg.eigvals(W_matrix)).real\n",
    "    else:\n",
    "        lambda_w = 0\n",
    "\n",
    "    # Store features\n",
    "    egonet_features[node] = {\n",
    "        \"N_i\": N_i,\n",
    "        \"E_i\": E_i,\n",
    "        \"W_i\": W_i,\n",
    "        \"lambda_w\": lambda_w,\n",
    "    }\n",
    "\n",
    "# Save the dictionary\n",
    "with open(\"egonet_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(egonet_features, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870b7e2",
   "metadata": {},
   "source": [
    "### Fitting power laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21baaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def power_law(x, C, alpha):\n",
    "    return C * np.power(x, alpha)\n",
    "\n",
    "def fit_powerlaw(x_vals, y_vals):\n",
    "    x = np.array(x_vals)\n",
    "    y = np.array(y_vals)\n",
    "    valid = (x > 2) & (y > 0)\n",
    "    x = x[valid]\n",
    "    y = y[valid]\n",
    "    popt, _ = curve_fit(power_law, x, y)\n",
    "    return popt  # returns C, alpha\n",
    "\n",
    "def outlier_score(y, y_hat):\n",
    "    return max(y, y_hat) / min(y, y_hat) * np.log(abs(y - y_hat) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e24945",
   "metadata": {},
   "source": [
    "### Counting outlier scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73389cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ni = [v[\"N_i\"] for v in egonet_features.values()]\n",
    "Ei = [v[\"E_i\"] for v in egonet_features.values()]\n",
    "C_edpl, alpha_edpl = fit_powerlaw(Ni, Ei)\n",
    "\n",
    "scores_edpl = {}\n",
    "for node, feats in egonet_features.items():\n",
    "    x = feats[\"N_i\"]\n",
    "    y = feats[\"E_i\"]\n",
    "    y_hat = power_law(x, C_edpl, alpha_edpl)\n",
    "    score = outlier_score(y, y_hat)\n",
    "    scores_edpl[node] = score\n",
    "\n",
    "\n",
    "# Save the dictionary\n",
    "with open(\"score_edpl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scores_edpl, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7104c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22314/878669918.py:16: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return max(y, y_hat) / min(y, y_hat) * np.log(abs(y - y_hat) + 1)\n"
     ]
    }
   ],
   "source": [
    "# Extract values\n",
    "E_vals = [v[\"E_i\"] for v in egonet_features.values()]\n",
    "W_vals = [v[\"W_i\"] for v in egonet_features.values()]\n",
    "\n",
    "# Fit power law\n",
    "C_ewpl, alpha_ewpl = fit_powerlaw(E_vals, W_vals)\n",
    "\n",
    "# Compute outlier scores\n",
    "scores_ewpl = {}\n",
    "for node, feats in egonet_features.items():\n",
    "    x = feats[\"E_i\"]\n",
    "    y = feats[\"W_i\"]\n",
    "    y_hat = power_law(x, C_ewpl, alpha_ewpl)\n",
    "    score = outlier_score(y, y_hat)\n",
    "    scores_ewpl[node] = score\n",
    "\n",
    "with open(\"score_ewpl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scores_ewpl, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d8101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22314/878669918.py:16: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return max(y, y_hat) / min(y, y_hat) * np.log(abs(y - y_hat) + 1)\n",
      "/tmp/ipykernel_22314/878669918.py:16: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return max(y, y_hat) / min(y, y_hat) * np.log(abs(y - y_hat) + 1)\n"
     ]
    }
   ],
   "source": [
    "W_vals = [v[\"W_i\"] for v in egonet_features.values()]\n",
    "L_vals = [v[\"lambda_w\"] for v in egonet_features.values()]\n",
    "\n",
    "# Fit power law\n",
    "C_elwpl, alpha_elwpl = fit_powerlaw(W_vals, L_vals)\n",
    "\n",
    "# Compute outlier scores\n",
    "scores_elwpl = {}\n",
    "for node, feats in egonet_features.items():\n",
    "    x = feats[\"W_i\"]\n",
    "    y = feats[\"lambda_w\"]\n",
    "    y_hat = power_law(x, C_elwpl, alpha_elwpl)\n",
    "    score = outlier_score(y, y_hat)\n",
    "    scores_elwpl[node] = score\n",
    "\n",
    "with open(\"score_elwpl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scores_elwpl, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37933c89",
   "metadata": {},
   "source": [
    "### Combine with LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98239899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samarth/.local/lib/python3.13/site-packages/sklearn/neighbors/_lof.py:322: UserWarning: Duplicate values are leading to incorrect results. Increase the number of neighbors for more accurate results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare features for LOF\n",
    "X_lof = np.array([[v[\"N_i\"], v[\"E_i\"], v[\"W_i\"], v[\"lambda_w\"]] for v in egonet_features.values()])\n",
    "lof_model = LocalOutlierFactor(n_neighbors=20, metric=\"euclidean\")\n",
    "lof_scores = -lof_model.fit_predict(X_lof)  # higher means more outlier\n",
    "\n",
    "# Normalize\n",
    "def safe_normalize(scores_dict):\n",
    "    vals = np.array(list(scores_dict.values()))\n",
    "    vals = np.nan_to_num(vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return MinMaxScaler().fit_transform(vals.reshape(-1, 1)).flatten()\n",
    "\n",
    "edpl_scores_norm = safe_normalize(scores_edpl)\n",
    "ewpl_scores_norm = safe_normalize(scores_ewpl)\n",
    "elwpl_scores_norm = safe_normalize(scores_elwpl)\n",
    "lof_scores_norm = safe_normalize(dict(zip(egonet_features.keys(), lof_scores)))\n",
    "\n",
    "\n",
    "# Combine\n",
    "combined_scores = {\n",
    "    node: edpl_scores_norm[i] + lof_scores_norm[i] + ewpl_scores_norm[i] + elwpl_scores_norm[i]\n",
    "    for i, node in enumerate(egonet_features.keys())\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67229340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "ip_labels = defaultdict(list)\n",
    "\n",
    "for chunk in pd.read_csv(file_path, usecols=[\"Src IP\", \"Dst IP\", \"Label\"], chunksize=chunk_size, low_memory=False):\n",
    "    chunk.dropna(subset=[\"Src IP\", \"Dst IP\"], inplace=True)\n",
    "    for _, row in chunk.iterrows():\n",
    "        ip_labels[row[\"Src IP\"]].append(row[\"Label\"])\n",
    "        ip_labels[row[\"Dst IP\"]].append(row[\"Label\"])\n",
    "\n",
    "# Final label per IP: use most common label\n",
    "ip_majority_label = {ip: Counter(labels).most_common(1)[0][0] for ip, labels in ip_labels.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54010444",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ip_majority_label.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ip_majority_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99ba5e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Node: 185.92.73.85, Combined Outlier Score: 2.6585\n",
      "2. Node: 23.36.32.43, Combined Outlier Score: 2.4592\n",
      "3. Node: 121.8.141.138, Combined Outlier Score: 2.3405\n",
      "4. Node: 210.206.216.138, Combined Outlier Score: 2.3405\n",
      "5. Node: 121.8.141.142, Combined Outlier Score: 2.3405\n",
      "6. Node: 58.63.230.158, Combined Outlier Score: 2.3405\n",
      "7. Node: 18.219.32.43, Combined Outlier Score: 2.0740\n",
      "8. Node: 52.14.136.135, Combined Outlier Score: 2.0700\n",
      "9. Node: 18.219.9.1, Combined Outlier Score: 2.0695\n",
      "10. Node: 18.216.200.189, Combined Outlier Score: 2.0678\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sort and view top 10\n",
    "sorted_outliers = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "top_outliers = sorted_outliers[:10]\n",
    "for i, (node, score) in enumerate(top_outliers):\n",
    "    print(f\"{i+1}. Node: {node}, Combined Outlier Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cabcf0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 9\n",
      "False Positives: 8\n"
     ]
    }
   ],
   "source": [
    "outlier_nodes = [node for node, _ in sorted_outliers[:17]]  \n",
    "\n",
    "\n",
    "true_positives = [ip for ip in outlier_nodes if ip_majority_label.get(ip, \"Benign\") != \"Benign\"]\n",
    "false_positives = [ip for ip in outlier_nodes if ip_majority_label.get(ip, \"Benign\") == \"Benign\"]\n",
    "\n",
    "print(f\"True Positives: {len(true_positives)}\")\n",
    "print(f\"False Positives: {len(false_positives)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc1eb75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5294\n",
      "Recall:    0.8182\n",
      "F1 Score:  0.6429\n"
     ]
    }
   ],
   "source": [
    "# Get all IPs labeled malicious\n",
    "malicious_ips = {ip for ip, label in ip_majority_label.items() if label != \"Benign\"}\n",
    "\n",
    "tp = len(set(outlier_nodes) & malicious_ips)\n",
    "fp = len(set(outlier_nodes) - malicious_ips)\n",
    "fn = len(malicious_ips - set(outlier_nodes))\n",
    "\n",
    "precision = tp / (tp + fp + 1e-6)\n",
    "recall = tp / (tp + fn + 1e-6)\n",
    "f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec1efed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier: 185.92.73.85 - Label: Benign\n",
      "Outlier: 23.36.32.43 - Label: Benign\n",
      "Outlier: 121.8.141.138 - Label: Benign\n",
      "Outlier: 210.206.216.138 - Label: Benign\n",
      "Outlier: 121.8.141.142 - Label: Benign\n",
      "Outlier: 58.63.230.158 - Label: Benign\n",
      "Outlier: 18.219.32.43 - Label: DDoS attacks-LOIC-HTTP\n",
      "Outlier: 52.14.136.135 - Label: DDoS attacks-LOIC-HTTP\n",
      "Outlier: 18.219.9.1 - Label: DDoS attacks-LOIC-HTTP\n",
      "Outlier: 18.216.200.189 - Label: DDoS attacks-LOIC-HTTP\n"
     ]
    }
   ],
   "source": [
    "for ip in top_outliers:\n",
    "    label = ip_majority_label.get(ip[0], \"UNKNOWN\")\n",
    "    print(f\"Outlier: {ip[0]} - Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208f6ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
